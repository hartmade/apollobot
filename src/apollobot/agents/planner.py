"""
Research Planner — the strategic reasoning layer.

Takes a Mission and produces a structured ResearchPlan that the
executor can follow phase by phase.  This is where the agent
"thinks like a scientist" about how to approach the problem.
"""

from __future__ import annotations

from typing import Any

from pydantic import BaseModel, Field, field_validator

from apollobot.agents import LLMProvider, LLMResponse
from apollobot.core.mission import Mission, ResearchMode
from apollobot.core.provenance import ProvenanceEngine


def _coerce_list(v: Any) -> list[str]:
    """Coerce a value to list[str] (handles LLM returning comma-separated strings)."""
    if isinstance(v, list):
        return [str(x) for x in v]
    if isinstance(v, str):
        return [s.strip() for s in v.split(",") if s.strip()]
    return [str(v)]


def _coerce_dict(v: Any) -> dict[str, Any]:
    """Coerce a value to dict (handles LLM returning strings instead of dicts)."""
    if isinstance(v, dict):
        return v
    if isinstance(v, str):
        return {"query": v}
    return {"value": v}


class DataRequirement(BaseModel):
    """A dataset or data source the plan needs."""

    description: str
    source_type: str  # mcp_server, download, generate
    server_name: str = ""
    query_params: dict[str, Any] = Field(default_factory=dict)
    priority: str = "required"  # required | nice_to_have
    fallback: str = ""

    @field_validator("query_params", mode="before")
    @classmethod
    def coerce_query_params(cls, v: Any) -> dict[str, Any]:
        return _coerce_dict(v)

    @field_validator("priority", mode="before")
    @classmethod
    def coerce_priority(cls, v: Any) -> str:
        return str(v)


class AnalysisStep(BaseModel):
    """A single analysis operation in the plan."""

    name: str
    description: str
    method: str  # e.g. "differential_expression", "linear_regression", "simulation"
    inputs: list[str] = Field(default_factory=list)  # references to data requirements
    parameters: dict[str, Any] = Field(default_factory=dict)
    expected_output: str = ""
    statistical_tests: list[str] = Field(default_factory=list)

    @field_validator("inputs", mode="before")
    @classmethod
    def coerce_inputs(cls, v: Any) -> list[str]:
        return _coerce_list(v)

    @field_validator("parameters", mode="before")
    @classmethod
    def coerce_parameters(cls, v: Any) -> dict[str, Any]:
        return _coerce_dict(v)

    @field_validator("statistical_tests", mode="before")
    @classmethod
    def coerce_statistical_tests(cls, v: Any) -> list[str]:
        return _coerce_list(v)


class ResearchPlan(BaseModel):
    """
    The complete research execution plan.
    Generated by the planner, executed by the runner.
    """

    mission_id: str
    summary: str
    approach: str  # narrative description of the research approach
    hypotheses: list[dict[str, str]] = Field(default_factory=list)  # {hypothesis, test, null_hypothesis}
    literature_queries: list[str] = Field(default_factory=list)
    data_requirements: list[DataRequirement] = Field(default_factory=list)
    analysis_steps: list[AnalysisStep] = Field(default_factory=list)
    statistical_framework: str = ""  # e.g. "frequentist with FDR correction"
    expected_outputs: list[str] = Field(default_factory=list)
    risks: list[str] = Field(default_factory=list)
    estimated_compute_cost: float = 0.0
    estimated_time_hours: float = 0.0

    @field_validator("risks", "expected_outputs", "literature_queries", mode="before")
    @classmethod
    def coerce_str_list(cls, v: Any) -> list[str]:
        if not isinstance(v, list):
            return _coerce_list(v)
        result = []
        for item in v:
            if isinstance(item, dict):
                # Flatten dict values into a single description string
                parts = [str(val) for val in item.values()]
                result.append(" — ".join(parts))
            else:
                result.append(str(item))
        return result

    @field_validator("estimated_compute_cost", "estimated_time_hours", mode="before")
    @classmethod
    def coerce_float(cls, v: Any) -> float:
        try:
            return float(v)
        except (ValueError, TypeError):
            return 0.0


# ---------------------------------------------------------------------------
# System prompts per research mode
# ---------------------------------------------------------------------------

PLANNER_SYSTEM = """You are a senior research scientist planning a computational study.
Given a research objective, you produce a detailed, executable research plan.

Your plan must be:
1. Scientifically rigorous — appropriate methods, proper controls, valid statistics
2. Reproducible — every step clearly defined with specific parameters
3. Falsifiable — for hypothesis mode, explicitly define what would disprove each hypothesis
4. Efficient — minimize compute and API costs while maintaining rigor
5. Honest — flag limitations, potential confounders, and risks upfront

You think carefully about what data exists, what analyses are appropriate,
and what conclusions can legitimately be drawn.
"""

MODE_PROMPTS = {
    ResearchMode.HYPOTHESIS: """
This is a HYPOTHESIS-DRIVEN study.  The researcher has specific claims to test.
Design the plan to:
- Clearly operationalize each hypothesis into testable predictions
- Define null hypotheses explicitly
- Choose appropriate statistical tests with pre-specified alpha levels
- Include power analysis where feasible
- ACTIVELY seek disconfirming evidence — design analyses that could falsify
- Plan for multiple comparison correction if testing multiple hypotheses
""",
    ResearchMode.EXPLORATORY: """
This is an EXPLORATORY study.  No prior hypothesis — we're mining for patterns.
Design the plan to:
- Cast a wide net with appropriate dimensionality reduction
- Apply STRICT multiple comparison correction (Bonferroni or FDR)
- Require cross-validation or hold-out validation for any discovered patterns
- Clearly label all findings as hypothesis-generating, not confirmatory
- Report effect sizes alongside significance
- Flag potential confounders aggressively
""",
    ResearchMode.META_ANALYSIS: """
This is a META-ANALYSIS / SYSTEMATIC REVIEW.
Design the plan to:
- Define clear inclusion/exclusion criteria for studies
- Plan systematic search across multiple literature databases
- Specify data extraction protocol
- Choose appropriate effect size measures and pooling methods
- Plan for heterogeneity assessment (I², Q-statistic)
- Include funnel plot / publication bias analysis
- Follow PRISMA guidelines
""",
    ResearchMode.REPLICATION: """
This is a REPLICATION study.  We're reproducing an existing paper's findings.
Design the plan to:
- Extract the original methodology precisely
- Reproduce analysis using the same data sources where possible
- Test sensitivity to analytical choices (multiverse analysis)
- Report both original and replicated results side by side
- Assess whether conclusions hold under alternative specifications
- Be fair but rigorous — note where replication succeeds AND fails
""",
    ResearchMode.SIMULATION: """
This is a SIMULATION / COMPUTATIONAL MODELING study.
Design the plan to:
- Define the model formally with all assumptions explicit
- Specify parameter ranges and sampling strategy
- Plan sensitivity analysis across key parameters
- Include validation against known analytical solutions or empirical data
- Design convergence checks
- Plan visualization of parameter space exploration
""",
}


class ResearchPlanner:
    """
    Generates a ResearchPlan from a Mission using LLM reasoning.
    """

    def __init__(self, llm: LLMProvider, provenance: ProvenanceEngine) -> None:
        self.llm = llm
        self.provenance = provenance

    async def plan(self, mission: Mission, available_servers: list[str] | None = None) -> ResearchPlan:
        """
        Generate a research plan for the given mission.
        """
        mode_prompt = MODE_PROMPTS.get(mission.mode, "")
        system = PLANNER_SYSTEM + mode_prompt

        user_prompt = self._build_prompt(mission, available_servers or [])

        self.provenance.log_event("planning_started", {"mission_id": mission.id})

        # First pass: generate the plan
        resp = await self.llm.complete_json(
            messages=[{"role": "user", "content": user_prompt}],
            system=system,
        )

        resp.pop("mission_id", None)
        plan = ResearchPlan(mission_id=mission.id, **resp)

        self.provenance.log_decision(
            description="Research plan generated",
            reasoning=plan.approach,
            alternatives=plan.risks,
        )

        # Second pass: self-critique
        critique = await self._critique_plan(mission, plan)
        if critique.get("issues"):
            self.provenance.log_event("plan_critique", {"issues": critique["issues"]})
            # Refine based on critique
            plan = await self._refine_plan(mission, plan, critique)

        self.provenance.log_event("planning_completed", {
            "num_data_requirements": len(plan.data_requirements),
            "num_analysis_steps": len(plan.analysis_steps),
            "estimated_cost": plan.estimated_compute_cost,
        })

        return plan

    def _build_prompt(self, mission: Mission, available_servers: list[str]) -> str:
        parts = [
            f"# Research Objective\n{mission.objective}",
        ]

        if mission.hypotheses:
            parts.append(f"# Hypotheses\n" + "\n".join(f"- {h}" for h in mission.hypotheses))

        if mission.paper_id:
            parts.append(f"# Paper to Replicate\n{mission.paper_id}")

        if mission.dataset_id:
            parts.append(f"# Target Dataset\n{mission.dataset_id}")

        parts.append(f"# Domain\n{mission.domain}")
        parts.append(f"# Research Mode\n{mission.mode.value}")
        parts.append(f"# Constraints\n"
                      f"- Compute budget: ${mission.constraints.compute_budget}\n"
                      f"- Time limit: {mission.constraints.time_limit}\n"
                      f"- Data sources: {mission.constraints.data_sources}\n"
                      f"- Ethics: {mission.constraints.ethics}")

        if available_servers:
            parts.append(f"# Available MCP Data Servers\n" + "\n".join(f"- {s}" for s in available_servers))

        parts.append(
            "\n# Instructions\n"
            "Produce a detailed research plan as a JSON object with these fields:\n"
            "- summary: one-paragraph summary of the plan\n"
            "- approach: detailed narrative of the research approach\n"
            "- hypotheses: list of {hypothesis, test, null_hypothesis} objects\n"
            "- literature_queries: list of search queries for literature review\n"
            "- data_requirements: list of {description, source_type, server_name, query_params, priority, fallback}\n"
            "- analysis_steps: list of {name, description, method, inputs, parameters, expected_output, statistical_tests}\n"
            "- statistical_framework: description of statistical approach\n"
            "- expected_outputs: list of expected deliverables\n"
            "- risks: list of risks and limitations\n"
            "- estimated_compute_cost: float (USD)\n"
            "- estimated_time_hours: float"
        )

        return "\n\n".join(parts)

    async def _critique_plan(self, mission: Mission, plan: ResearchPlan) -> dict[str, Any]:
        """Self-critique: find weaknesses in the plan."""
        resp = await self.llm.complete_json(
            messages=[{"role": "user", "content": (
                f"Critique this research plan for the objective: {mission.objective}\n\n"
                f"Plan summary: {plan.summary}\n"
                f"Approach: {plan.approach}\n"
                f"Statistical framework: {plan.statistical_framework}\n\n"
                "Identify issues with:\n"
                "1. Statistical validity\n"
                "2. Potential confounders not addressed\n"
                "3. Missing controls\n"
                "4. Data quality concerns\n"
                "5. Logical gaps in reasoning\n"
                "6. Alternative approaches that might be better\n\n"
                "Return JSON with: {issues: [...], severity: 'low'|'medium'|'high', suggestions: [...]}"
            )}],
            system="You are a skeptical peer reviewer. Find real problems, not nitpicks.",
        )
        return resp

    async def _refine_plan(
        self, mission: Mission, plan: ResearchPlan, critique: dict[str, Any]
    ) -> ResearchPlan:
        """Refine plan based on self-critique."""
        resp = await self.llm.complete_json(
            messages=[{"role": "user", "content": (
                f"Original plan:\n{plan.model_dump_json(indent=2)}\n\n"
                f"Critique:\n{json.dumps(critique, indent=2)}\n\n"
                "Revise the plan to address these issues. Return the complete "
                "revised plan as JSON with the same schema."
            )}],
            system=PLANNER_SYSTEM,
        )
        resp.pop("mission_id", None)
        return ResearchPlan(mission_id=mission.id, **resp)


import json  # noqa: E402 (needed for _refine_plan)
